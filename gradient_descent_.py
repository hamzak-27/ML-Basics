# -*- coding: utf-8 -*-
"""gradient descent .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r_rmOKfG3kv95iz9EJI7KL2WL3xwLFds
"""

from sklearn.datasets import make_regression
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

X,y = make_regression(n_samples = 4,n_features=1,n_informative=1,n_targets = 1,noise = 80,random_state=13)

plt.scatter(X,y)
plt.show()

from sklearn.linear_model import LinearRegression
reg = LinearRegression()
reg.fit(X,y)

reg.coef_

reg.intercept_

#we applied OLS just to find out the value of m and b

#we now apply gradient descent with constant slope = 78.35 and starting b=0
y_pred = ((78.35*X)+0).reshape(4)

plt.scatter(X,y)
plt.plot(X,reg.predict(X),color = 'red',label = 'OLS')
plt.plot(X,y_pred,color = 'black',label='b=0')
plt.legend()
plt.show()

#So for red line b value is 26.15 and for black line b=0. Now we will apply gradient descent and you will see that the black line will move towards red line.

m=78.35
b=0

loss_slope = -2*np.sum(y - m*X.ravel() - b)
print(loss_slope)

#learning rate

lr = 0.1
step_size = loss_slope*lr
print(step_size)

#calculating the new interept
b = b - step_size
print(b)

y_pred1 = ((78.35*X)+b).reshape(4)

plt.scatter(X,y)
plt.plot(X,reg.predict(X),color = 'red',label = 'OLS')
plt.plot(X,y_pred1,color = 'blue',label = "b={}".format(b))
plt.plot(X,y_pred,color = 'black',label='b=0')
plt.legend()
plt.show()

#Iteration 2

loss_slope = -2*np.sum(y - m*X.ravel() - b)
print(loss_slope)

step_size = loss_slope*lr
print(step_size)

b = b - step_size
print(b)

y_pred1 = ((78.35*X)+b).reshape(4)

plt.scatter(X,y)
plt.plot(X,reg.predict(X),color = 'red',label = 'OLS')
plt.plot(X,y_pred1,color = 'blue',label = "b={}".format(b))
plt.plot(X,y_pred,color = 'black',label='b=0')
plt.legend()
plt.show()

#Using loops for this

b = 0
m = 78.35
lr = 0.1

epochs = 10

for i in range(epochs):
  loss_slope = -2*np.sum(y - m*X.ravel() - b)
  step_size = loss_slope*lr
  b = b - step_size

  y_pred = m*X + b

  plt.plot(X,y_pred)

plt.scatter(X,y)

